{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "# Load the CSV containing the printabels\n",
    "label_file = os.path.join('label.csv')  # Adjust the path as needed\n",
    "df = pd.read_csv(label_file)\n",
    "\n",
    "# Print the first few rows of the label file\n",
    "print(df.head())\n",
    "\n",
    "# 1. Count number of images per class\n",
    "class_distribution = df['label'].value_counts()\n",
    "print(\"\\nNumber of images per class:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# 2. Visualize the distribution of images per class\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=class_distribution.index, y=class_distribution.values, palette='viridis')\n",
    "plt.title(\"Distribution of Images per Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 3. Check image sizes by directly analyzing images\n",
    "image_folder = '/kaggle/input/data222/data'  # The folder where images are stored\n",
    "image_sizes = []\n",
    "\n",
    "for img_name in df['filename']:  # Assuming 'filename' is a column in the CSV\n",
    "    img_path = os.path.join(image_folder, img_name)  # Path to image\n",
    "    \n",
    "    try:\n",
    "        # Open the image using PIL and append the size (width, height)\n",
    "        with Image.open(img_path) as img:\n",
    "            image_sizes.append(img.size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {img_name}: {e}\")\n",
    "\n",
    "# Convert image sizes to a DataFrame for statistics\n",
    "size_df = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\n",
    "print(\"\\nImage Size Statistics:\")\n",
    "print(size_df.describe())\n",
    "\n",
    "# 4. Visualize image sizes (width and height)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(size_df['Width'], bins=20, kde=True, color='blue')\n",
    "plt.title(\"Distribution of Image Widths\")\n",
    "plt.xlabel(\"Width\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(size_df['Height'], bins=20, kde=True, color='green')\n",
    "plt.title(\"Distribution of Image Heights\")\n",
    "plt.xlabel(\"Height\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "total_root = \"/kaggle/input/data222/data\"\n",
    "\n",
    "\n",
    "# Load the CSV containing the labels\n",
    "label_file = os.path.join('/kaggle/input/data222/label.csv')  # Adjust the filename as needed\n",
    "df = pd.read_csv(label_file)\n",
    "\n",
    "# Print the first few rows of the label file\n",
    "print(df.head())\n",
    "\n",
    "# 1. Count number of images per class\n",
    "class_distribution = df['label'].value_counts()\n",
    "print(\"\\nNumber of images per class:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# 2. Visualize the distribution of images per class\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=class_distribution.index, y=class_distribution.values, palette='viridis')\n",
    "plt.title(\"Distribution of Images per Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 3. Display a Single Sample Image from Each Class\n",
    "num_samples = 7  # Number of samples to display per class\n",
    "plt.figure(figsize=(20, len(class_distribution) * 3))\n",
    "\n",
    "for i, cls in enumerate(class_distribution.index):\n",
    "    class_images = df[df['label'] == cls]['filename'].tolist()\n",
    "    sample_images = random.sample(class_images, min(num_samples, len(class_images)))\n",
    "    \n",
    "    for j, img_name in enumerate(sample_images):\n",
    "        img_path = os.path.join(total_root, img_name)\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            plt.subplot(len(class_distribution), num_samples, i * num_samples + j + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(cls)\n",
    "            plt.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV containing the labels\n",
    "label_file = os.path.join('//kaggle/input/data222/label.csv')  # Adjust the filename as needed\n",
    "df = pd.read_csv(label_file)\n",
    "\n",
    "# 1. Count number of images per class\n",
    "class_distribution = df['label'].value_counts()\n",
    "print(\"\\nNumber of images per class:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# 2. Visualize the distribution of images per class\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=class_distribution.index, y=class_distribution.values, palette='viridis')\n",
    "plt.title(\"Distribution of Images per Class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 3. Identify Class Imbalances\n",
    "total_images = class_distribution.sum()\n",
    "class_percentages = (class_distribution / total_images) * 100\n",
    "print(\"\\nClass Percentages:\")\n",
    "print(class_percentages)\n",
    "\n",
    "# Discuss Class Imbalances\n",
    "threshold = 10  # Define a threshold percentage for imbalance\n",
    "imbalanced_classes = class_percentages[class_percentages < threshold]\n",
    "if not imbalanced_classes.empty:\n",
    "    print(\"\\nImbalanced Classes (less than {}% of total images):\".format(threshold))\n",
    "    print(imbalanced_classes)\n",
    "else:\n",
    "    print(\"\\nNo significant class imbalances detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv\n",
    "%pip instasll mahotas opencv\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mahotas\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "total_root = \"/kaggle/input/data222/data\"\n",
    "\n",
    "\n",
    "# Load the CSV containing the labels\n",
    "label_file = os.path.join('/kaggle/input/data222/label.csv')  # Adjust the filename as needed\n",
    "df = pd.read_csv(label_file)\n",
    "\n",
    "# Initialize lists to store features\n",
    "hog_features = []\n",
    "color_histograms = []\n",
    "glcm_features = []\n",
    "\n",
    "# Define a function to extract HOG features using OpenCV\n",
    "def extract_hog_features(image):\n",
    "    winSize = (64, 64)\n",
    "    blockSize = (16, 16)\n",
    "    blockStride = (8, 8)\n",
    "    cellSize = (8, 8)\n",
    "    nbins = 9\n",
    "    hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)\n",
    "    h = hog.compute(image)\n",
    "    return h.flatten()\n",
    "\n",
    "# Define a function to extract color histogram\n",
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    # Compute a 3D color histogram in the RGB color space\n",
    "    hist = cv2.calcHist([image], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    # Normalize the histogram\n",
    "    hist = cv2.normalize(hist, hist).flatten()\n",
    "    return hist\n",
    "\n",
    "# Define a function to extract GLCM features using Mahotas\n",
    "def extract_glcm_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    textures = mahotas.features.haralick(gray_image).mean(axis=0)\n",
    "    return textures\n",
    "\n",
    "# Iterate through the images and extract features\n",
    "for img_name in df['filename']:\n",
    "    img_path = os.path.join(total_root, img_name)\n",
    "    image = cv2.imread(img_path)\n",
    "    \n",
    "    # Extract HOG features\n",
    "    hog_feature = extract_hog_features(image)\n",
    "    hog_features.append(hog_feature)\n",
    "    \n",
    "    # Extract color histogram\n",
    "    color_histogram = extract_color_histogram(image)\n",
    "    color_histograms.append(color_histogram)\n",
    "    \n",
    "    # Extract GLCM features\n",
    "    glcm_feature = extract_glcm_features(image)\n",
    "    glcm_features.append(glcm_feature)\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "hog_df = pd.DataFrame(hog_features)\n",
    "color_hist_df = pd.DataFrame(color_histograms)\n",
    "glcm_df = pd.DataFrame(glcm_features, columns=['Angular Second Moment', 'Contrast', 'Correlation', 'Variance', 'Inverse Difference Moment', 'Sum Average', 'Sum Variance', 'Sum Entropy', 'Entropy', 'Difference Variance', 'Difference Entropy', 'Information Measure of Correlation 1', 'Information Measure of Correlation 2', 'Maximal Correlation Coefficient'])\n",
    "\n",
    "# Print feature DataFrames\n",
    "print(\"HOG Features:\")\n",
    "print(hog_df.head())\n",
    "\n",
    "print(\"\\nColor Histogram Features:\")\n",
    "print(color_hist_df.head())\n",
    "\n",
    "print(\"\\nGLCM Features:\")\n",
    "print(glcm_df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
